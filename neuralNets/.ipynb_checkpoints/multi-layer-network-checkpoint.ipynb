{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Multilayer Neural Network\n",
    "\n",
    "The objective of this section is to implement the backpropagation algorithm (SGD with the chain rule) on a two layer (one hidden and one output) neural network using the sigmoid activation function.\n",
    "\n",
    "http://cs231n.github.io/neural-networks-case-study/#net is a good resource for this though note they setup the problem slightly differently and use a different activation function. Use it for help working out the gradients which requires some careful work.\n",
    "\n",
    "Some initial imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "def one_hot(n_classes, y):\n",
    "    return np.eye(n_classes)[y]\n",
    "\n",
    "\n",
    "data = np.asarray(digits.data, dtype='float32')\n",
    "target = np.asarray(digits.target, dtype='int32')\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.15, random_state=37)\n",
    "\n",
    "# Normalise the images\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "Y_test = one_hot(10, y_test)\n",
    "Y_train = one_hot(10, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement the `sigmoid` and its element-wise derivative `dsigmoid` functions:\n",
    "\n",
    "$$\n",
    "sigmoid(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "dsigmoid(x) = sigmoid(x) \\cdot (1 - sigmoid(x))\n",
    "$$\n",
    "\n",
    "Plot both the sigmoid and its derivative and compare it to wikipedia or wolframalpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-c41e361615d2>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-c41e361615d2>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    return dsigmoid(X).(1-sigmoid(X))\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(X):\n",
    "    return 1/(1+exp(-X))\n",
    "\n",
    "\n",
    "def dsigmoid(X):\n",
    "    return dsigmoid(X).(1-sigmoid(X))\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "plt.plot(x, sigmoid(x), label='sigmoid')\n",
    "plt.plot(x, dsigmoid(x), label='dsigmoid')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement `forward` and `forward_keep_all` functions for a model with a hidden layer, similar to what we discussed in the slides:\n",
    "  - $h = sigmoid(\\mathbf{W}^h x + b^h)$\n",
    "  - $y = softmax(\\mathbf{W}^o h + b^o)$\n",
    "\n",
    "Notes: \n",
    "  - try to keep the code as similar as possible as for `LogisticRegression`;\n",
    "  - `forward_keep_activations` is similar to forward, but also returns hidden activations and pre activations;\n",
    "\n",
    "- update the grad function to compute all gradients; check that the gradients are well defined;\n",
    "\n",
    "- implement the `train` and `loss` functions.\n",
    "- for efficiency reasons we need to compute the gradients analytically but we can compare our results to those of a numerical computation to check we got it right: http://cs231n.github.io/optimization-1/#gradcompute pretty helpful in debugging. Use it on small sub-expressions to build up the complete gradient expression\n",
    "\n",
    "Bonus: reimplementing all from scratch without looking at the solution of the `LogisticRegression` is an excellent exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some useful functions from before\n",
    "EPSILON = 1e-8\n",
    "\n",
    "\n",
    "def nll(Y_true, Y_pred):\n",
    "    Y_true, Y_pred = np.atleast_2d(Y_true), np.atleast_2d(Y_pred)\n",
    "    loglikelihoods = np.sum(np.log(EPSILON + Y_pred) * Y_true, axis=1)\n",
    "    return -np.mean(loglikelihoods)\n",
    "\n",
    "def softmax(X):\n",
    "    exp = np.exp(X)\n",
    "    return exp / np.sum(exp, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNet():\n",
    "    \"\"\"MLP with 1 hidden layer with a sigmoid activation\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.W_h = np.random.uniform(\n",
    "            size=(input_size, hidden_size), high=0.01, low=-0.01)\n",
    "        self.b_h = np.zeros(hidden_size)\n",
    "        self.W_o = np.random.uniform(\n",
    "            size=(hidden_size, output_size), high=0.01, low=-0.01)\n",
    "        self.b_o =np.zeros(output_size)\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # TODO: implement the forward step\n",
    "        return None\n",
    "    \n",
    "    def forward_keep_activations(self, X):\n",
    "        # TODO: compute the forward step and return intermediate results as well\n",
    "        z_h = None # output of hidden layer pre activation\n",
    "        h = None # output of hidden layer post actiation\n",
    "        z_o = None # output of final layer pre activation\n",
    "        y = None # output of final layer post activation\n",
    "        return y, h, z_h\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        # TODO: compute the loss of samples X given true labels y\n",
    "        return None\n",
    "\n",
    "    def grad_loss(self, x, y_true):\n",
    "        # TODO: compute the gradient of sample x with respect to the two Ws and bs\n",
    "        y, h, z_h = self.forward_keep_activations(x)\n",
    "        grad_z_o = y - one_hot(self.output_size, y_true)\n",
    "\n",
    "        grad_W_o = np.outer(h, grad_z_o)\n",
    "        grad_b_o = grad_z_o\n",
    "        grad_h = np.dot(grad_z_o, np.transpose(self.W_o))\n",
    "        grad_z_h = grad_h * dsigmoid(z_h)\n",
    "        grad_W_h = np.outer(x, grad_z_h)\n",
    "        grad_b_h = grad_z_h\n",
    "        grads = {\"W_h\": grad_W_h, \"b_h\": grad_b_h,\n",
    "                 \"W_o\": grad_W_o, \"b_o\": grad_b_o}\n",
    "        return grads\n",
    "\n",
    "    def train(self, x, y, learning_rate):\n",
    "        # Traditional SGD update on one sample at a time\n",
    "        grads = self.grad_loss(x, y)\n",
    "        self.W_h = self.W_h - learning_rate * grads[\"W_h\"]\n",
    "        self.b_h = self.b_h - learning_rate * grads[\"b_h\"]\n",
    "        self.W_o = self.W_o - learning_rate * grads[\"W_o\"]\n",
    "        self.b_o = self.b_o - learning_rate * grads[\"b_o\"]\n",
    "\n",
    "    def predict(self, X):\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis=1)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_preds = np.argmax(self.forward(X), axis=1)\n",
    "        return np.mean(y_preds == y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = X_train.shape[1]\n",
    "n_classes = Y_train.shape[1]\n",
    "\n",
    "n_hidden = 10\n",
    "model = NeuralNet(n_features, n_hidden, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do things look like before training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.loss(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.accuracy(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_idx = 0\n",
    "plt.plot(model.forward(X_train[sample_idx]), linestyle='-', label='prediction')\n",
    "plt.plot(one_hot(10, y_train[sample_idx]), linestyle='--', label='true')\n",
    "plt.title('output probabilities')\n",
    "plt.legend()\n",
    "print(model.predict(X_train[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses, accuracies, accuracies_test = [], [], []\n",
    "losses.append(model.loss(X_train, y_train))\n",
    "accuracies.append(model.accuracy(X_train, y_train))\n",
    "accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "\n",
    "print(\"Random init: train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\"\n",
    "      % (losses[-1], accuracies[-1], accuracies_test[-1]))\n",
    "\n",
    "for epoch in range(15):\n",
    "    for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "        model.train(x, y, 0.1)\n",
    "\n",
    "    losses.append(model.loss(X_train, y_train))\n",
    "    accuracies.append(model.accuracy(X_train, y_train))\n",
    "    accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "    print(\"Epoch #%d, train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\"\n",
    "          % (epoch + 1, losses[-1], accuracies[-1], accuracies_test[-1]))\n",
    "    \n",
    "# rerun the notebook without normalising/scaling the images and performance will be worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "96% testing accuracy! Not bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.title(\"Training loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(accuracies, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Hyper parameters settings\n",
    "\n",
    "- Experiment with different hyper parameters:\n",
    "  - learning rate,\n",
    "  - size of hidden layer,\n",
    "  - initialization scheme: test with 0 initialization vs uniform,\n",
    "  - implement other activation functions,\n",
    "  - implement the support for a second hidden layer.\n",
    "\n",
    "\n",
    "### Mini-batches\n",
    "\n",
    "- the current implementations of the `train` and `grad_loss` functions currently only accept a single sample at a time:\n",
    "    - implement the support for training with a mini-batch of 32 samples at a time instead of one,\n",
    "    - experiment with different sizes of batches,\n",
    "\n",
    "\n",
    "### Performance\n",
    "\n",
    "- can you get to 97% accuracy by tweaking the hyper-parameters?\n",
    "\n",
    "### Momentum\n",
    "\n",
    "- (bonus) Implement a optimizer with momentum\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Build up your intuition\n",
    "\n",
    "What is the minimal number of neurons and layers needed to classify the circle dataset defined below?\n",
    "\n",
    "What is the smallest NN to solve the spiral dataset defined below?\n",
    "\n",
    "In each case can you visualise what the network is doing? Can you explain why your answer is the answer?\n",
    "\n",
    "Take a look at https://github.com/wildtreetech/advanced-comp-2017/blob/master/03-neural-networks/neural-network-transformations.ipynb to get you started on answering the \"why\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_spiral():\n",
    "    N = 100 # number of points per class\n",
    "    K = 3 # number of classes\n",
    "    X = np.zeros((N*K, 2)) # data matrix (each row = single example)\n",
    "    y = np.zeros(N*K, dtype='uint8') # class labels\n",
    "    for j in range(K):\n",
    "        ix = range(N*j, N*(j+1))\n",
    "        r = np.linspace(0.0, 1, N) # radius\n",
    "        t = np.linspace(j*4, (j+1)*4, N) + np.random.randn(N)*0.2 # theta\n",
    "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        y[ix] = j\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "def make_circle_data():\n",
    "    X, y = make_circles(n_samples=400, factor=.3, noise=.1)\n",
    "    labels = ['b', 'r']\n",
    "    y = np.take(labels, (y < 0.5))\n",
    "    return X, y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
