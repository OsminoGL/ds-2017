{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "In this section we will implement a logistic regression model trainable with SGD using numpy. Here are the objectives:\n",
    "\n",
    "1. Implement a simple forward model with no hidden layer (equivalent to logistic regression):\n",
    "$y = softmax(\\mathbf{W} x + b)$\n",
    "\n",
    "1. build a `predict` function which returns the most probable class given an input $x$\n",
    "\n",
    "1. build an `accuracy` function for a batch of inputs $X$ and the corresponding expected outputs $y_{true}$\n",
    "\n",
    "1. build a `grad` function which computes $\\frac{d}{dW} -\\log(softmax(Wx + b))$ for an $x$ and its corresponding expected output $y_{true}$ ; check that the gradients are well defined\n",
    "\n",
    "1. build a `train` function which uses the `grad` function output to update $\\mathbf{W}$ and $b$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's define a helper function to compute the one hot encoding of an integer array for a fixed number of classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(n_classes, y):\n",
    "    return np.eye(n_classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_hot(10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_hot(10, [3,2,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_hot(10, [0, 4, 9, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a moment to take a look at the dataset before we start using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_index = 42 # change this to see different examples\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(digits.images[sample_index], cmap=plt.cm.gray_r,\n",
    "           interpolation='nearest')\n",
    "plt.title(\"image label: %d\" % digits.target[sample_index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "- normalization (for more take a look at http://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "- train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "data = np.asarray(digits.data, dtype='float32')\n",
    "target = np.asarray(digits.target, dtype='int32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.15, random_state=37)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "Y_train = one_hot(10, y_train)\n",
    "Y_test = one_hot(10, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = one_hot(10, y_train)\n",
    "Y_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the one of the transformed samples (after feature standardization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_index = 45\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(X_train[sample_index].reshape(8, 8),\n",
    "           cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.title(\"transformed sample\\n(standardised)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaler object makes it possible to recover the original sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(scaler.inverse_transform(X_train[sample_index]).reshape(8, 8),\n",
    "           cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.title(\"original sample\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement the softmax vector function:\n",
    "\n",
    "$$\n",
    "softmax(\\mathbf{x}) = \\frac{1}{\\sum_{i=1}^{n}{e^{x_i}}}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "  e^{x_1}\\\\\\\\\n",
    "  e^{x_2}\\\\\\\\\n",
    "  \\vdots\\\\\\\\\n",
    "  e^{x_n}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    # TODO:\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that this works one vector at a time (and check that the components sum to one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(softmax([10, 2, -3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a naive implementation of softmax might not be able process a batch of activations in a single call (but we need that):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[10, 2, -3],\n",
    "              [-1, 5, -20]])\n",
    "print(softmax(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that given the true one-hot encoded class `Y_true` and and some predicted probabilities `Y_pred` returns the negative log likelihood. For this problem we are using the \"log loss\" loss function (also known as cross-entropy).\n",
    "\n",
    "This `nll` is a scalar computed using all samples in our training set. The likelihood measures how likely something is. Using the term \"likelihood\" instead of probability because the likelihood does not have to be a probability.\n",
    "\n",
    "$$L = P_1 \\cdot P_2 \\cdot P_3 \\cdot ... $$\n",
    "\n",
    "However multiplying lots of small numbers together is a numerical disaster. Use $\\log(ab) = \\log(a) + log(b)$ to convert the product into a sum (hence the name log-likelihood). The negative just means we multiply everything by minus one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPSILON = 1e-8 # this is here to give you a hint on how to deal with the case when y_pred=0\n",
    "\n",
    "def nll(Y_true, Y_pred):\n",
    "    # TODO\n",
    "    return None\n",
    "\n",
    "# Make sure that it works for a simple sample at a time\n",
    "print(nll([1, 0, 0], [.99, 0.01, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the `nll` of a very confident yet incorrect prediction is a much higher positive number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(nll([1, 0, 0], [0.01, 0.01, .98]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that your implementation can compute the average negative log likelihood of a group of predictions: `Y_pred` and `Y_true` can therefore be passed in as 2D arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nll(Y_true, Y_pred):\n",
    "    Y_true, Y_pred = np.atleast_2d(Y_true), np.atleast_2d(Y_pred)\n",
    "    loglikelihoods = np.sum(np.log(EPSILON + Y_pred) * Y_true, axis=1)\n",
    "    return -np.mean(loglikelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check that the average NLL of the following 3 almost perfect\n",
    "# predictions is close to 0\n",
    "Y_true = np.array([[0, 1, 0],\n",
    "                   [1, 0, 0],\n",
    "                   [0, 0, 1]])\n",
    "\n",
    "Y_pred = np.array([[0,   1,    0],\n",
    "                   [.99, 0.01, 0],\n",
    "                   [0,   0,    1]])\n",
    "\n",
    "print(nll(Y_true, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have all the ingredients for training a logistic regression model using gradient descent.\n",
    "\n",
    "Let's study it **one sample at a time**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.W = np.random.uniform(size=(input_size, output_size),\n",
    "                                   high=0.1, low=-0.1)\n",
    "        self.b = np.random.uniform(size=output_size,\n",
    "                                   high=0.1, low=-0.1)\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # TODO: compute normalised scores for each class\n",
    "        return None\n",
    "\n",
    "    def predict(self, X):\n",
    "        # TODO: for each sample in X return the predicted class\n",
    "        return None\n",
    "    \n",
    "    def grad_loss(self, x, y_true):\n",
    "        # TODO: compute gradient with respect to W and b for a sample x\n",
    "        # and the true label y_true\n",
    "        y_pred = self.forward(x)\n",
    "\n",
    "        grads = {\"W\": grad_W, \"b\": grad_b}\n",
    "        return grads\n",
    "    \n",
    "    def train(self, x, y, learning_rate):\n",
    "        # TODO: compute one step of traditional gradient descent update without momentum\n",
    "        # and update W and b\n",
    "        grads = self.grad_loss(x, y)\n",
    "        \n",
    "    def loss(self, x, y):\n",
    "        # TODO: use `nll` to compute the loss for the sample x with true label y\n",
    "        return None\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        # TODO: compute accuracy for samples X with true labels y\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a model and test its forward inference\n",
    "n_features = X_train.shape[1]\n",
    "n_classes = Y_train.shape[1]\n",
    "lr = LogisticRegression(n_features, n_classes)\n",
    "\n",
    "print(\"Evaluation of the untrained model:\")\n",
    "train_loss = lr.loss(X_train, y_train)\n",
    "train_acc = lr.accuracy(X_train, y_train)\n",
    "test_acc = lr.accuracy(X_train, y_train)\n",
    "\n",
    "print(\"train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\"\n",
    "      % (train_loss, train_acc, test_acc))\n",
    "# Question: do you think the accuracy makes sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the untrained model on an example\n",
    "sample_idx = 3\n",
    "plt.plot(lr.forward(X_train[sample_idx]), linestyle='-', label='prediction')\n",
    "plt.plot(one_hot(10, y_train[sample_idx]), linestyle='--', label='true')\n",
    "plt.title('output probabilities')\n",
    "plt.legend()\n",
    "print(lr.predict(X_train[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training for one epoch\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "    lr.train(x, y, learning_rate)\n",
    "    if i % 100 == 0:\n",
    "        train_loss = lr.loss(X_train, y_train)\n",
    "        train_acc = lr.accuracy(X_train, y_train)\n",
    "        test_acc = lr.accuracy(X_test, y_test)\n",
    "        print(\"Update #%d, train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\"\n",
    "              % (i, train_loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the trained model on an example\n",
    "sample_idx = 899\n",
    "plt.plot(lr.forward(X_train[sample_idx]), linestyle='-', label='prediction')\n",
    "plt.plot(one_hot(10, y_train[sample_idx]), linestyle='--', label='true')\n",
    "plt.title('output probabilities')\n",
    "plt.legend()\n",
    "print(lr.predict(X_train[sample_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "* can you find examples that are mispredicted, is there a pattern to the wrong predictions?\n",
    "* visualise these samples and their predicted classes\n",
    "* plot the [confusion matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) to find classes that are hard to separate (maybe eight vs nine?)\n",
    "* do things improve with training for more epochs?\n",
    "* experiment with different values of the learning rate. Can you accelerate learning? What happens for very large values of the learning rate? Very small ones?\n",
    "* convert the training setup to use stochastic gradient descent, does this change things?\n",
    "* what is the optimal number of epochs to train for? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
