{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "logistic regression model trainable with SGD using numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### First let's define a helper function to compute the one hot encoding of an integer array for a fixed number of classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(n_classes, y):\n",
    "    return np.eye(n_classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(10, [3,2,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(10, [0, 4, 9, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a moment to take a look at the dataset before we start using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMQAAADaCAYAAAD5eu2AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvzoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10889ff98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_index = 42 # change this to see different examples\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(digits.images[sample_index], cmap=plt.cm.gray_r,\n",
    "           interpolation='nearest')\n",
    "plt.title(\"image label: %d\" % digits.target[sample_index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "- normalization (for more take a look at http://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "- train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "data = np.asarray(digits.data, dtype='float32')\n",
    "target = np.asarray(digits.target, dtype='int32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.15, random_state=37)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "Y_train = one_hot(10, y_train)\n",
    "Y_test = one_hot(10, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = one_hot(10, y_train)\n",
    "Y_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the one of the transformed samples (after feature standardization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMQAAADvCAYAAACtzXueAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWBDGFFgQxhRYEMYUWBDGFFgQxhT8P5GmZlP8IWA1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1088b1eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_index = 45\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(X_train[sample_index].reshape(8, 8),\n",
    "           cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.title(\"transformed sample\\n(standardised)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaler object makes it possible to recover the original sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMQAAADaCAYAAAD5eu2AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZpn/BwlHDoSha3PZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10866bb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(scaler.inverse_transform(X_train[sample_index]).reshape(8, 8),\n",
    "           cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.title(\"original sample\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement the softmax vector function:\n",
    "\n",
    "$$\n",
    "softmax(\\mathbf{x}) = \\frac{1}{\\sum_{i=1}^{n}{e^{x_i}}}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "  e^{x_1}\\\\\\\\\n",
    "  e^{x_2}\\\\\\\\\n",
    "  \\vdots\\\\\\\\\n",
    "  e^{x_n}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    return (np.exp(X)/np.sum(np.exp(X), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that this works one vector at a time (and check that the components sum to one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(softmax([10, 8, -3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a naive implementation of softmax might not be able process a batch of activations in a single call (but we need that):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.04740913  0.95259087]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[10, 2, -3],\n",
    "              [-1, 5, -20]])\n",
    "print(np.sum(softmax(X), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that given the true one-hot encoded class `Y_true` and and some predicted probabilities `Y_pred` returns the negative log likelihood. For this problem we are using the \"log loss\" loss function (also known as cross-entropy).\n",
    "\n",
    "This `nll` is a scalar computed using all samples in our training set. The likelihood measures how likely something is. Using the term \"likelihood\" instead of probability because the likelihood does not have to be a probability.\n",
    "\n",
    "$$L = P_1 \\cdot P_2 \\cdot P_3 \\cdot ... $$\n",
    "\n",
    "However multiplying lots of small numbers together is a numerical disaster. Use $\\log(ab) = \\log(a) + log(b)$ to convert the product into a sum (hence the name log-likelihood). The negative just means we multiply everything by minus one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.70225530858\n"
     ]
    }
   ],
   "source": [
    "EPSILON = 1e-8 # this is here to give you a hint on how to deal with the case when y_pred=0\n",
    "#L y, f (x)  = log 1 + exp(−yf (x) \n",
    "def nll(Y_true, Y_pred):\n",
    "    return np.sum((np.log(1 + np.exp(-Y_pred*Y_true))))\n",
    "\n",
    "# Make sure that it works for a simple sample at a time\n",
    "print(nll(np.array([1, 0, 0]), np.array([.99, 0.01, 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the `nll` of a very confident yet incorrect prediction is a much higher positive number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.07445404163\n"
     ]
    }
   ],
   "source": [
    "print(nll(np.array([1, 0, 0]), np.array([0.01, 0.01, .98])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that your implementation can compute the average negative log likelihood of a group of predictions: `Y_pred` and `Y_true` can therefore be passed in as 2D arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nll(Y_true, Y_pred):\n",
    "    Y_true, Y_pred = np.atleast_2d(Y_true), np.atleast_2d(Y_pred)\n",
    "    loglikelihoods = np.sum(np.log(EPSILON + Y_pred) * Y_true, axis=1)\n",
    "    return -np.mean(loglikelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0033501019175\n"
     ]
    }
   ],
   "source": [
    "# Check that the average NLL of the following 3 almost perfect\n",
    "# predictions is close to 0\n",
    "Y_true = np.array([[0, 1, 0],\n",
    "                   [1, 0, 0],\n",
    "                   [0, 0, 1]])\n",
    "\n",
    "Y_pred = np.array([[0,   1,    0],\n",
    "                   [.99, 0.01, 0],\n",
    "                   [0,   0,    1]])\n",
    "\n",
    "print(nll(Y_true, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have all the ingredients for training a logistic regression model using gradient descent.\n",
    "\n",
    "Let's study it **one sample at a time**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.W = np.random.uniform(size=(input_size, output_size),\n",
    "                                   high=0.1, low=-0.1)\n",
    "        self.b = np.random.uniform(size=output_size,\n",
    "                                   high=0.1, low=-0.1)\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # y=softmax(Wx+b)      \n",
    "        return softmax(np.dot(X, self.W) + self.b)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # for each sample in X return the predicted class\n",
    "        y_pred = self.forward(X)\n",
    "        return y_pred\n",
    "    \n",
    "    def grad_loss(self, x, y_true):\n",
    "        # TODO: compute gradient with respect to W and b for a sample x\n",
    "        # and the true label y_true\n",
    "        # ddW−log(softmax(Wx+b))\n",
    "        y_pred = self.forward(x)\n",
    "        \n",
    "        grad_W = - 2 * y_pred * (y_true - np.log(softmax(self.W * y_pred) + self.b))\n",
    "        grad_b = 2 * (y_true - np.log(softmax(self.W*y_pred) + self.b))\n",
    "    \n",
    "        grads = {\"W\": grad_W, \"b\": grad_b}\n",
    "        return grads\n",
    "    \n",
    "    def train(self, x, y, learning_rate):\n",
    "        # TODO: compute one step of traditional gradient descent update without momentum\n",
    "        # and update W and b\n",
    "        grads = self.grad_loss(x, y)\n",
    "        \n",
    "        self.W = self.W - (learning_rate * grads['W'])\n",
    "        self.b = self.b - (learning_rate * grads['b'])\n",
    "        \n",
    "    def loss(self, x, y):\n",
    "        # TODO: use `nll` to compute the loss for the sample x with true label y\n",
    "        y_pred = nll(x, y)\n",
    "        loss = (y - y_pred)**2\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        # TODO: compute accuracy for samples X with true labels y\n",
    "        acc_ = self.loss(X, y)    \n",
    "        acc = np.sum(acc_)/len(X)\n",
    "\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of the untrained model:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,1527) (1527,64) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-5e34cbae400a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluation of the untrained model:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-61-71c4ebc7e982>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# TODO: use `nll` to compute the loss for the sample x with true label y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-89f1db220949>\u001b[0m in \u001b[0;36mnll\u001b[0;34m(Y_true, Y_pred)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mY_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mloglikelihoods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPSILON\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mY_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloglikelihoods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,1527) (1527,64) "
     ]
    }
   ],
   "source": [
    "# Build a model and test its forward inference\n",
    "n_features = X_train.shape[1]\n",
    "n_classes = Y_train.shape[1]\n",
    "lr = LogisticRegression(n_features, n_classes)\n",
    "\n",
    "print(\"Evaluation of the untrained model:\")\n",
    "train_loss = lr.loss(X_train, y_train)\n",
    "train_acc = lr.accuracy(X_train, y_train)\n",
    "test_acc = lr.accuracy(X_train, y_train)\n",
    "\n",
    "print(\"train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\"\n",
    "      % (train_loss, train_acc, test_acc))\n",
    "# Question: do you think the accuracy makes sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the untrained model on an example\n",
    "sample_idx = 3\n",
    "plt.plot(lr.forward(X_train[sample_idx]), linestyle='-', label='prediction')\n",
    "plt.plot(one_hot(10, y_train[sample_idx]), linestyle='--', label='true')\n",
    "plt.title('output probabilities')\n",
    "plt.legend()\n",
    "print(lr.predict(X_train[sample_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training for one epoch\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "    lr.train(x, y, learning_rate)\n",
    "    if i % 100 == 0:\n",
    "        train_loss = lr.loss(X_train, y_train)\n",
    "        train_acc = lr.accuracy(X_train, y_train)\n",
    "        test_acc = lr.accuracy(X_test, y_test)\n",
    "        print(\"Update #%d, train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\"\n",
    "              % (i, train_loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the trained model on an example\n",
    "sample_idx = 899\n",
    "plt.plot(lr.forward(X_train[sample_idx]), linestyle='-', label='prediction')\n",
    "plt.plot(one_hot(10, y_train[sample_idx]), linestyle='--', label='true')\n",
    "plt.title('output probabilities')\n",
    "plt.legend()\n",
    "print(lr.predict(X_train[sample_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "* can you find examples that are mispredicted, is there a pattern to the wrong predictions?\n",
    "* visualise these samples and their predicted classes\n",
    "* plot the [confusion matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) to find classes that are hard to separate (maybe eight vs nine?)\n",
    "* do things improve with training for more epochs?\n",
    "* experiment with different values of the learning rate. Can you accelerate learning? What happens for very large values of the learning rate? Very small ones?\n",
    "* convert the training setup to use stochastic gradient descent, does this change things?\n",
    "* what is the optimal number of epochs to train for? Why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
